{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Python for Research Homework: Week 4, Case Study 3\n",
    "\n",
    "Homophily is a property of networks.  Homophily occurs when nodes that are neighbors in a network also share a characteristic more often than nodes that are not network neighbors.  In this case study, we will investigate homophily of several characteristics of individuals connected in social networks in rural India."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "In Exercise 1, we will calculate the chance homophily for an arbitrary characteristic. Homophily is the proportion of edges in the network whose constituent nodes share that characteristic.\n",
    "\n",
    "How much homophily do we expect by chance? If characteristics are distributed completely randomly, the probability that two nodes \\(x\\) and \\(y\\) share characteristic \\(a\\) is the probability both nodes have characteristic \\(a\\) , which is the marginal probability of \\(a\\) squared. The total probability that nodes \\(x\\) and \\(y\\) share their characteristic is therefore the sum of the square of the marginal probabilities of each characteristic in the network.\n",
    "\n",
    "<strong>Instructions</strong>\n",
    "<ul><li>Create a function <code>marginal_prob</code> that takes a dictionary <code>chars</code> with personal IDs as keys and characteristics as values; it should return a dictionary with characteristics as keys and their marginal probability (frequency of occurence of a characteristic divided by the sum of frequencies of each characteristic) as values.</li>\n",
    "<li>Create a function <code>chance_homophily(chars)</code> that takes a dictionary <code>chars</code> defined as above and computes the chance homophily (homophily due to chance alone) for that characteristic.</li>\n",
    "<li>A sample of three peoples' favorite colors is given in <code>favorite_colors</code>. Use your function to compute the chance homophily in this group, and store it as <code>color_homophily</code>.</li>\n",
    "<li>Print <code>color_homophily</code>.</li></ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5555555555555556\n",
      "   village  adjmatrix_key     pid  hhid  resp_id  resp_gend  \\\n",
      "0        1              5  100201  1002        1          1   \n",
      "1        1              6  100202  1002        2          2   \n",
      "2        1             23  100601  1006        1          1   \n",
      "3        1             24  100602  1006        2          2   \n",
      "4        1             27  100701  1007        1          1   \n",
      "\n",
      "                   resp_status  age  religion caste  ...       privategovt  \\\n",
      "0            Head of Household   38  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "1  Spouse of Head of Household   27  HINDUISM   OBC  ...               NaN   \n",
      "2            Head of Household   29  HINDUISM   OBC  ...        OTHER LAND   \n",
      "3  Spouse of Head of Household   24  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "4            Head of Household   58  HINDUISM   OBC  ...        OTHER LAND   \n",
      "\n",
      "  work_outside work_outside_freq shgparticipate shg_no savings savings_no  \\\n",
      "0          Yes               0.0             No    NaN      No        NaN   \n",
      "1          NaN               NaN             No    NaN      No        NaN   \n",
      "2           No               NaN             No    NaN      No        NaN   \n",
      "3           No               NaN            Yes    1.0     Yes        1.0   \n",
      "4           No               NaN             No    NaN      No        NaN   \n",
      "\n",
      "  electioncard rationcard rationcard_colour  \n",
      "0          Yes        Yes             GREEN  \n",
      "1          Yes        Yes             GREEN  \n",
      "2          Yes        Yes             GREEN  \n",
      "3          Yes         No               NaN  \n",
      "4          Yes        Yes             GREEN  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "def marginal_prob(chars):\n",
    "    # Count the occurrences of each characteristic\n",
    "    count = Counter(chars.values())\n",
    "    \n",
    "    # Calculate the total number of characteristics\n",
    "    total_count = sum(count.values())\n",
    "    \n",
    "    # Calculate the marginal probability for each characteristic\n",
    "    marginal_probs = {char: freq / total_count for char, freq in count.items()}\n",
    "\n",
    "    return marginal_probs\n",
    "        \n",
    "def chance_homophily(chars):\n",
    "    marginal_probs = marginal_prob(chars)\n",
    "    \n",
    "    # Calculate the chance homophily by summing the squares of the marginal probabilities\n",
    "    homophily = sum(prob ** 2 for prob in marginal_probs.values())\n",
    "    \n",
    "    return homophily\n",
    "\n",
    "favorite_colors = {\n",
    "    \"ankit\":  \"red\",\n",
    "    \"xiaoyu\": \"blue\",\n",
    "    \"mary\":   \"blue\"\n",
    "}\n",
    "\n",
    "color_homophily = chance_homophily(favorite_colors)\n",
    "print(color_homophily)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "\n",
    "In the remaining exercises, we will calculate actual homophily in these village and compare the obtained values to those obtained by chance. In this exercise, we subset the data into individual villages and store them.\n",
    "\n",
    "#### Instructions \n",
    "\n",
    "- `individual_characteristics.dta` contains several characteristics for each individual in the dataset such as age, religion, and caste. Use the `pandas` library to read in and store these characteristics as a dataframe called `df`.\n",
    "- Store separate datasets for individuals belonging to Villages 1 and 2 as `df1` and `df2`, respectively.\n",
    "- Note that some attributes may be missing for some individuals. In this case study, we will ignore rows of data where some column information is missing.\n",
    "- Use the head method to display the first few entries of `df1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   village  adjmatrix_key     pid  hhid  resp_id  resp_gend  \\\n",
      "0        1              5  100201  1002        1          1   \n",
      "1        1              6  100202  1002        2          2   \n",
      "2        1             23  100601  1006        1          1   \n",
      "3        1             24  100602  1006        2          2   \n",
      "4        1             27  100701  1007        1          1   \n",
      "\n",
      "                   resp_status  age  religion caste  ...       privategovt  \\\n",
      "0            Head of Household   38  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "1  Spouse of Head of Household   27  HINDUISM   OBC  ...               NaN   \n",
      "2            Head of Household   29  HINDUISM   OBC  ...        OTHER LAND   \n",
      "3  Spouse of Head of Household   24  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "4            Head of Household   58  HINDUISM   OBC  ...        OTHER LAND   \n",
      "\n",
      "  work_outside work_outside_freq shgparticipate shg_no savings savings_no  \\\n",
      "0          Yes               0.0             No    NaN      No        NaN   \n",
      "1          NaN               NaN             No    NaN      No        NaN   \n",
      "2           No               NaN             No    NaN      No        NaN   \n",
      "3           No               NaN            Yes    1.0     Yes        1.0   \n",
      "4           No               NaN             No    NaN      No        NaN   \n",
      "\n",
      "  electioncard rationcard rationcard_colour  \n",
      "0          Yes        Yes             GREEN  \n",
      "1          Yes        Yes             GREEN  \n",
      "2          Yes        Yes             GREEN  \n",
      "3          Yes         No               NaN  \n",
      "4          Yes        Yes             GREEN  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "     village  adjmatrix_key     pid  hhid  resp_id  resp_gend  \\\n",
      "203        2              1  200101  2001        1          1   \n",
      "204        2              6  200201  2002        1          1   \n",
      "205        2              7  200202  2002        2          2   \n",
      "206        2             12  200401  2004        1          1   \n",
      "207        2             13  200402  2004        2          2   \n",
      "\n",
      "                     resp_status  age  religion    caste  ... privategovt  \\\n",
      "203            Head of Household   33  HINDUISM  GENERAL  ...      OTHERS   \n",
      "204            Head of Household   35  HINDUISM  GENERAL  ...      OTHERS   \n",
      "205  Spouse of Head of Household   30  HINDUISM  GENERAL  ...      OTHERS   \n",
      "206            Head of Household   42  HINDUISM  GENERAL  ...      OTHERS   \n",
      "207  Spouse of Head of Household   39  HINDUISM  GENERAL  ...         NaN   \n",
      "\n",
      "    work_outside work_outside_freq shgparticipate shg_no savings savings_no  \\\n",
      "203          Yes               0.0             No    NaN      No        NaN   \n",
      "204           No               NaN             No    NaN      No        NaN   \n",
      "205           No               NaN            Yes    1.0     Yes        1.0   \n",
      "206           No               NaN             No    NaN     Yes        1.0   \n",
      "207          NaN               NaN            Yes    1.0      No        NaN   \n",
      "\n",
      "    electioncard rationcard rationcard_colour  \n",
      "203          Yes        Yes            YELLOW  \n",
      "204          Yes        Yes            YELLOW  \n",
      "205          Yes        Yes            YELLOW  \n",
      "206          Yes        Yes            YELLOW  \n",
      "207          Yes        Yes            YELLOW  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "   village  adjmatrix_key     pid  hhid  resp_id  resp_gend  \\\n",
      "0        1              5  100201  1002        1          1   \n",
      "1        1              6  100202  1002        2          2   \n",
      "2        1             23  100601  1006        1          1   \n",
      "3        1             24  100602  1006        2          2   \n",
      "4        1             27  100701  1007        1          1   \n",
      "\n",
      "                   resp_status  age  religion caste  ...       privategovt  \\\n",
      "0            Head of Household   38  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "1  Spouse of Head of Household   27  HINDUISM   OBC  ...               NaN   \n",
      "2            Head of Household   29  HINDUISM   OBC  ...        OTHER LAND   \n",
      "3  Spouse of Head of Household   24  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "4            Head of Household   58  HINDUISM   OBC  ...        OTHER LAND   \n",
      "\n",
      "  work_outside work_outside_freq shgparticipate shg_no savings savings_no  \\\n",
      "0          Yes               0.0             No    NaN      No        NaN   \n",
      "1          NaN               NaN             No    NaN      No        NaN   \n",
      "2           No               NaN             No    NaN      No        NaN   \n",
      "3           No               NaN            Yes    1.0     Yes        1.0   \n",
      "4           No               NaN             No    NaN      No        NaN   \n",
      "\n",
      "  electioncard rationcard rationcard_colour  \n",
      "0          Yes        Yes             GREEN  \n",
      "1          Yes        Yes             GREEN  \n",
      "2          Yes        Yes             GREEN  \n",
      "3          Yes         No               NaN  \n",
      "4          Yes        Yes             GREEN  \n",
      "\n",
      "[5 rows x 48 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df  = pd.read_csv(\"https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@individual_characteristics.csv\", low_memory=False, index_col=0)\n",
    "# Filter the dataset for Village 1\n",
    "df1 = df[df['village'] == 1]\n",
    "\n",
    "# Filter the dataset for Village 2\n",
    "df2 = df[df['village'] == 2]\n",
    "\n",
    "# Display the first few entries of df1\n",
    "print(df1.head())\n",
    "print(df2.head())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3 \n",
    "\n",
    "In this exercise, we define a few dictionaries that enable us to look up the sex, caste, and religion of members of each village by personal ID. For Villages 1 and 2, their personal IDs are stored as `pid`.\n",
    "\n",
    "#### Instructions \n",
    "- Define dictionaries with personal IDs as keys and a given covariate for that individual as values. Complete this for the sex, caste, and religion covariates, for Villages 1 and 2.\n",
    "- For Village 1, store these dictionaries into variables named `sex1`, `caste1`, and `religion1`.\n",
    "- For Village 2, store these dictionaries into variables named `sex2`, `caste2`, and `religion2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   village  adjmatrix_key     pid  hhid  resp_id  resp_gend  \\\n",
      "0        1              5  100201  1002        1          1   \n",
      "1        1              6  100202  1002        2          2   \n",
      "2        1             23  100601  1006        1          1   \n",
      "3        1             24  100602  1006        2          2   \n",
      "4        1             27  100701  1007        1          1   \n",
      "\n",
      "                   resp_status  age  religion caste  ...       privategovt  \\\n",
      "0            Head of Household   38  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "1  Spouse of Head of Household   27  HINDUISM   OBC  ...               NaN   \n",
      "2            Head of Household   29  HINDUISM   OBC  ...        OTHER LAND   \n",
      "3  Spouse of Head of Household   24  HINDUISM   OBC  ...  PRIVATE BUSINESS   \n",
      "4            Head of Household   58  HINDUISM   OBC  ...        OTHER LAND   \n",
      "\n",
      "  work_outside work_outside_freq shgparticipate shg_no savings savings_no  \\\n",
      "0          Yes               0.0             No    NaN      No        NaN   \n",
      "1          NaN               NaN             No    NaN      No        NaN   \n",
      "2           No               NaN             No    NaN      No        NaN   \n",
      "3           No               NaN            Yes    1.0     Yes        1.0   \n",
      "4           No               NaN             No    NaN      No        NaN   \n",
      "\n",
      "  electioncard rationcard rationcard_colour  \n",
      "0          Yes        Yes             GREEN  \n",
      "1          Yes        Yes             GREEN  \n",
      "2          Yes        Yes             GREEN  \n",
      "3          Yes         No               NaN  \n",
      "4          Yes        Yes             GREEN  \n",
      "\n",
      "[5 rows x 48 columns]\n",
      "Sex1: {0: 1, 1: 2, 2: 1, 3: 2, 4: 1, 5: 2, 6: 1, 7: 2, 8: 2, 9: 1, 10: 1, 11: 2, 12: 2, 13: 2, 14: 2, 15: 1, 16: 2, 17: 1, 18: 2, 19: 2, 20: 1, 21: 2, 22: 1, 23: 2, 24: 1, 25: 2, 26: 1, 27: 2, 28: 1, 29: 2, 30: 2, 31: 2, 32: 1, 33: 2, 34: 1, 35: 2, 36: 1, 37: 2, 38: 1, 39: 2, 40: 1, 41: 2, 42: 1, 43: 2, 44: 1, 45: 2, 46: 1, 47: 2, 48: 1, 49: 2, 50: 1, 51: 2, 52: 1, 53: 1, 54: 2, 55: 1, 56: 2, 57: 1, 58: 2, 59: 2, 60: 2, 61: 1, 62: 2, 63: 1, 64: 2, 65: 1, 66: 2, 67: 1, 68: 1, 69: 2, 70: 1, 71: 2, 72: 2, 73: 1, 74: 2, 75: 1, 76: 2, 77: 1, 78: 2, 79: 1, 80: 2, 81: 1, 82: 2, 83: 1, 84: 2, 85: 2, 86: 2, 87: 1, 88: 1, 89: 1, 90: 2, 91: 2, 92: 1, 93: 2, 94: 2, 95: 1, 96: 2, 97: 1, 98: 2, 99: 1, 100: 2, 101: 1, 102: 2, 103: 1, 104: 2, 105: 1, 106: 2, 107: 1, 108: 2, 109: 1, 110: 2, 111: 1, 112: 2, 113: 1, 114: 2, 115: 1, 116: 2, 117: 1, 118: 2, 119: 1, 120: 2, 121: 2, 122: 2, 123: 2, 124: 1, 125: 1, 126: 2, 127: 1, 128: 2, 129: 1, 130: 2, 131: 1, 132: 2, 133: 1, 134: 2, 135: 2, 136: 1, 137: 1, 138: 2, 139: 1, 140: 2, 141: 2, 142: 1, 143: 2, 144: 1, 145: 2, 146: 1, 147: 2, 148: 2, 149: 1, 150: 2, 151: 1, 152: 2, 153: 1, 154: 2, 155: 2, 156: 1, 157: 2, 158: 1, 159: 2, 160: 1, 161: 2, 162: 1, 163: 2, 164: 1, 165: 1, 166: 1, 167: 2, 168: 1, 169: 2, 170: 2, 171: 2, 172: 1, 173: 2, 174: 1, 175: 2, 176: 1, 177: 2, 178: 1, 179: 2, 180: 1, 181: 2, 182: 1, 183: 2, 184: 1, 185: 2, 186: 1, 187: 2, 188: 1, 189: 2, 190: 1, 191: 2, 192: 2, 193: 1, 194: 2, 195: 2, 196: 1, 197: 1, 198: 2, 199: 2, 200: 1, 201: 2, 202: 1}\n",
      "Caste1: {0: 'OBC', 1: 'OBC', 2: 'OBC', 3: 'OBC', 4: 'OBC', 5: 'OBC', 6: 'OBC', 7: 'OBC', 8: 'OBC', 9: 'OBC', 10: 'OBC', 11: 'OBC', 12: 'SCHEDULED CASTE', 13: 'SCHEDULED CASTE', 14: 'SCHEDULED CASTE', 15: 'SCHEDULED CASTE', 16: 'SCHEDULED CASTE', 17: 'OBC', 18: 'OBC', 19: 'OBC', 20: 'OBC', 21: 'OBC', 22: 'OBC', 23: 'OBC', 24: 'OBC', 25: 'OBC', 26: 'OBC', 27: 'OBC', 28: 'OBC', 29: 'OBC', 30: 'OBC', 31: 'OBC', 32: 'SCHEDULED CASTE', 33: 'SCHEDULED CASTE', 34: 'SCHEDULED CASTE', 35: 'SCHEDULED CASTE', 36: 'SCHEDULED CASTE', 37: 'SCHEDULED CASTE', 38: 'SCHEDULED CASTE', 39: 'SCHEDULED CASTE', 40: 'OBC', 41: 'OBC', 42: 'OBC', 43: 'OBC', 44: 'OBC', 45: 'OBC', 46: 'OBC', 47: 'OBC', 48: 'OBC', 49: 'OBC', 50: 'OBC', 51: 'OBC', 52: 'OBC', 53: 'OBC', 54: 'OBC', 55: 'OBC', 56: 'OBC', 57: 'GENERAL', 58: 'GENERAL', 59: 'GENERAL', 60: 'OBC', 61: 'OBC', 62: 'OBC', 63: 'OBC', 64: 'OBC', 65: 'OBC', 66: 'OBC', 67: 'OBC', 68: 'OBC', 69: 'OBC', 70: 'OBC', 71: 'OBC', 72: 'OBC', 73: 'OBC', 74: 'OBC', 75: 'OBC', 76: 'OBC', 77: 'OBC', 78: 'OBC', 79: 'OBC', 80: 'OBC', 81: 'OBC', 82: 'OBC', 83: 'OBC', 84: 'OBC', 85: 'OBC', 86: 'OBC', 87: 'OBC', 88: 'OBC', 89: 'OBC', 90: 'OBC', 91: 'GENERAL', 92: 'GENERAL', 93: 'GENERAL', 94: 'OBC', 95: 'OBC', 96: 'OBC', 97: 'OBC', 98: 'OBC', 99: 'OBC', 100: 'OBC', 101: 'OBC', 102: 'OBC', 103: 'OBC', 104: 'OBC', 105: 'OBC', 106: 'OBC', 107: 'OBC', 108: 'OBC', 109: 'OBC', 110: 'OBC', 111: 'OBC', 112: 'OBC', 113: 'OBC', 114: 'OBC', 115: 'OBC', 116: 'OBC', 117: 'OBC', 118: 'OBC', 119: 'OBC', 120: 'OBC', 121: 'OBC', 122: 'OBC', 123: 'OBC', 124: 'OBC', 125: 'OBC', 126: 'OBC', 127: 'OBC', 128: 'OBC', 129: 'OBC', 130: 'OBC', 131: 'OBC', 132: 'OBC', 133: 'OBC', 134: 'GENERAL', 135: 'OBC', 136: 'GENERAL', 137: 'OBC', 138: 'OBC', 139: 'OBC', 140: 'OBC', 141: 'OBC', 142: 'OBC', 143: 'OBC', 144: 'SCHEDULED CASTE', 145: 'SCHEDULED CASTE', 146: 'OBC', 147: 'OBC', 148: 'OBC', 149: 'OBC', 150: 'SCHEDULED CASTE', 151: 'SCHEDULED CASTE', 152: 'SCHEDULED CASTE', 153: 'SCHEDULED CASTE', 154: 'SCHEDULED CASTE', 155: 'SCHEDULED CASTE', 156: 'SCHEDULED CASTE', 157: 'SCHEDULED CASTE', 158: 'SCHEDULED CASTE', 159: 'SCHEDULED CASTE', 160: 'OBC', 161: 'SCHEDULED TRIBE', 162: 'OBC', 163: 'OBC', 164: 'SCHEDULED CASTE', 165: 'GENERAL', 166: 'GENERAL', 167: 'OBC', 168: 'OBC', 169: 'OBC', 170: 'OBC', 171: 'OBC', 172: 'OBC', 173: 'OBC', 174: 'OBC', 175: 'OBC', 176: 'OBC', 177: 'OBC', 178: 'OBC', 179: 'OBC', 180: 'OBC', 181: 'OBC', 182: 'OBC', 183: 'OBC', 184: 'OBC', 185: 'OBC', 186: 'OBC', 187: 'OBC', 188: 'OBC', 189: 'OBC', 190: 'OBC', 191: 'OBC', 192: 'OBC', 193: 'OBC', 194: 'OBC', 195: 'OBC', 196: 'OBC', 197: 'OBC', 198: 'OBC', 199: 'OBC', 200: 'OBC', 201: 'SCHEDULED CASTE', 202: 'SCHEDULED CASTE'}\n",
      "Religion1: {0: 'HINDUISM', 1: 'HINDUISM', 2: 'HINDUISM', 3: 'HINDUISM', 4: 'HINDUISM', 5: 'HINDUISM', 6: 'HINDUISM', 7: 'HINDUISM', 8: 'HINDUISM', 9: 'HINDUISM', 10: 'HINDUISM', 11: 'HINDUISM', 12: 'HINDUISM', 13: 'HINDUISM', 14: 'HINDUISM', 15: 'HINDUISM', 16: 'HINDUISM', 17: 'HINDUISM', 18: 'HINDUISM', 19: 'HINDUISM', 20: 'HINDUISM', 21: 'HINDUISM', 22: 'HINDUISM', 23: 'HINDUISM', 24: 'HINDUISM', 25: 'HINDUISM', 26: 'HINDUISM', 27: 'HINDUISM', 28: 'HINDUISM', 29: 'HINDUISM', 30: 'HINDUISM', 31: 'HINDUISM', 32: 'HINDUISM', 33: 'HINDUISM', 34: 'HINDUISM', 35: 'HINDUISM', 36: 'HINDUISM', 37: 'HINDUISM', 38: 'HINDUISM', 39: 'HINDUISM', 40: 'HINDUISM', 41: 'HINDUISM', 42: 'HINDUISM', 43: 'HINDUISM', 44: 'ISLAM', 45: 'ISLAM', 46: 'HINDUISM', 47: 'HINDUISM', 48: 'HINDUISM', 49: 'HINDUISM', 50: 'HINDUISM', 51: 'HINDUISM', 52: 'HINDUISM', 53: 'HINDUISM', 54: 'HINDUISM', 55: 'HINDUISM', 56: 'HINDUISM', 57: 'HINDUISM', 58: 'HINDUISM', 59: 'HINDUISM', 60: 'HINDUISM', 61: 'HINDUISM', 62: 'HINDUISM', 63: 'HINDUISM', 64: 'HINDUISM', 65: 'HINDUISM', 66: 'HINDUISM', 67: 'HINDUISM', 68: 'HINDUISM', 69: 'HINDUISM', 70: 'HINDUISM', 71: 'HINDUISM', 72: 'HINDUISM', 73: 'HINDUISM', 74: 'HINDUISM', 75: 'HINDUISM', 76: 'HINDUISM', 77: 'HINDUISM', 78: 'HINDUISM', 79: 'HINDUISM', 80: 'HINDUISM', 81: 'HINDUISM', 82: 'HINDUISM', 83: 'HINDUISM', 84: 'HINDUISM', 85: 'HINDUISM', 86: 'HINDUISM', 87: 'HINDUISM', 88: 'HINDUISM', 89: 'HINDUISM', 90: 'HINDUISM', 91: 'HINDUISM', 92: 'HINDUISM', 93: 'HINDUISM', 94: 'HINDUISM', 95: 'HINDUISM', 96: 'HINDUISM', 97: 'HINDUISM', 98: 'HINDUISM', 99: 'HINDUISM', 100: 'HINDUISM', 101: 'HINDUISM', 102: 'HINDUISM', 103: 'HINDUISM', 104: 'HINDUISM', 105: 'HINDUISM', 106: 'HINDUISM', 107: 'HINDUISM', 108: 'HINDUISM', 109: 'HINDUISM', 110: 'HINDUISM', 111: 'HINDUISM', 112: 'HINDUISM', 113: 'HINDUISM', 114: 'HINDUISM', 115: 'HINDUISM', 116: 'HINDUISM', 117: 'HINDUISM', 118: 'HINDUISM', 119: 'HINDUISM', 120: 'HINDUISM', 121: 'HINDUISM', 122: 'HINDUISM', 123: 'HINDUISM', 124: 'HINDUISM', 125: 'HINDUISM', 126: 'HINDUISM', 127: 'HINDUISM', 128: 'HINDUISM', 129: 'HINDUISM', 130: 'HINDUISM', 131: 'HINDUISM', 132: 'HINDUISM', 133: 'HINDUISM', 134: 'HINDUISM', 135: 'HINDUISM', 136: 'HINDUISM', 137: 'HINDUISM', 138: 'HINDUISM', 139: 'HINDUISM', 140: 'HINDUISM', 141: 'HINDUISM', 142: 'HINDUISM', 143: 'HINDUISM', 144: 'HINDUISM', 145: 'HINDUISM', 146: 'HINDUISM', 147: 'HINDUISM', 148: 'HINDUISM', 149: 'HINDUISM', 150: 'HINDUISM', 151: 'HINDUISM', 152: 'HINDUISM', 153: 'HINDUISM', 154: 'HINDUISM', 155: 'HINDUISM', 156: 'HINDUISM', 157: 'HINDUISM', 158: 'HINDUISM', 159: 'HINDUISM', 160: 'HINDUISM', 161: 'HINDUISM', 162: 'HINDUISM', 163: 'HINDUISM', 164: 'HINDUISM', 165: 'HINDUISM', 166: 'HINDUISM', 167: 'HINDUISM', 168: 'HINDUISM', 169: 'HINDUISM', 170: 'HINDUISM', 171: 'HINDUISM', 172: 'HINDUISM', 173: 'HINDUISM', 174: 'HINDUISM', 175: 'HINDUISM', 176: 'HINDUISM', 177: 'HINDUISM', 178: 'HINDUISM', 179: 'HINDUISM', 180: 'HINDUISM', 181: 'HINDUISM', 182: 'HINDUISM', 183: 'HINDUISM', 184: 'HINDUISM', 185: 'HINDUISM', 186: 'HINDUISM', 187: 'HINDUISM', 188: 'HINDUISM', 189: 'HINDUISM', 190: 'HINDUISM', 191: 'HINDUISM', 192: 'HINDUISM', 193: 'HINDUISM', 194: 'HINDUISM', 195: 'HINDUISM', 196: 'HINDUISM', 197: 'HINDUISM', 198: 'HINDUISM', 199: 'HINDUISM', 200: 'HINDUISM', 201: 'HINDUISM', 202: 'HINDUISM'}\n",
      "Sex2: {203: 1, 204: 1, 205: 2, 206: 1, 207: 2, 208: 1, 209: 2, 210: 1, 211: 2, 212: 1, 213: 2, 214: 1, 215: 2, 216: 2, 217: 2, 218: 1, 219: 2, 220: 2, 221: 1, 222: 2, 223: 1, 224: 1, 225: 2, 226: 1, 227: 2, 228: 1, 229: 2, 230: 1, 231: 2, 232: 1, 233: 2, 234: 1, 235: 2, 236: 1, 237: 2, 238: 1, 239: 2, 240: 1, 241: 2, 242: 2, 243: 1, 244: 2, 245: 1, 246: 2, 247: 1, 248: 2, 249: 1, 250: 2, 251: 1, 252: 2, 253: 2, 254: 1, 255: 2, 256: 1, 257: 2, 258: 2, 259: 1, 260: 2, 261: 1, 262: 2, 263: 1, 264: 2, 265: 1, 266: 2, 267: 1, 268: 2, 269: 1, 270: 2, 271: 1, 272: 2, 273: 1, 274: 2, 275: 1, 276: 2, 277: 1, 278: 2, 279: 1, 280: 2, 281: 1, 282: 2, 283: 1, 284: 1, 285: 2, 286: 1, 287: 2, 288: 1, 289: 1, 290: 2, 291: 1, 292: 2, 293: 1, 294: 2, 295: 2, 296: 1, 297: 2, 298: 1, 299: 2, 300: 1, 301: 1, 302: 1, 303: 2, 304: 1, 305: 2, 306: 2, 307: 1, 308: 2, 309: 1, 310: 2, 311: 1, 312: 2, 313: 1, 314: 2, 315: 1, 316: 2, 317: 1, 318: 2, 319: 1, 320: 2, 321: 2, 322: 1, 323: 2, 324: 1, 325: 2, 326: 1, 327: 2, 328: 1, 329: 1, 330: 2, 331: 1, 332: 2, 333: 1, 334: 2, 335: 1, 336: 2, 337: 1, 338: 2, 339: 2, 340: 1, 341: 2, 342: 1, 343: 2, 344: 1, 345: 2, 346: 2, 347: 1, 348: 2, 349: 1, 350: 1, 351: 2, 352: 1, 353: 2, 354: 1, 355: 2, 356: 1, 357: 2, 358: 1, 359: 2, 360: 1, 361: 2, 362: 1, 363: 2, 364: 1, 365: 2, 366: 1, 367: 2, 368: 1, 369: 2, 370: 2, 371: 2, 372: 1, 373: 2, 374: 1, 375: 2, 376: 2, 377: 2, 378: 1, 379: 2, 380: 1, 381: 2, 382: 1, 383: 2, 384: 1, 385: 2, 386: 1, 387: 2, 388: 1, 389: 2, 390: 1, 391: 2, 392: 1, 393: 2, 394: 1, 395: 2, 396: 1, 397: 2, 398: 1, 399: 2, 400: 1, 401: 2, 402: 1, 403: 2, 404: 2, 405: 1}\n",
      "Caste2: {203: 'GENERAL', 204: 'GENERAL', 205: 'GENERAL', 206: 'GENERAL', 207: 'GENERAL', 208: 'GENERAL', 209: 'GENERAL', 210: 'GENERAL', 211: 'GENERAL', 212: 'GENERAL', 213: 'OBC', 214: 'GENERAL', 215: 'GENERAL', 216: 'GENERAL', 217: 'GENERAL', 218: 'GENERAL', 219: 'GENERAL', 220: 'GENERAL', 221: 'GENERAL', 222: 'GENERAL', 223: 'GENERAL', 224: 'GENERAL', 225: 'GENERAL', 226: 'OBC', 227: 'OBC', 228: 'GENERAL', 229: 'GENERAL', 230: 'GENERAL', 231: 'GENERAL', 232: 'GENERAL', 233: 'GENERAL', 234: 'GENERAL', 235: 'GENERAL', 236: 'GENERAL', 237: 'GENERAL', 238: 'GENERAL', 239: 'GENERAL', 240: 'GENERAL', 241: 'OBC', 242: 'OBC', 243: 'GENERAL', 244: 'GENERAL', 245: 'GENERAL', 246: 'GENERAL', 247: 'OBC', 248: 'OBC', 249: 'GENERAL', 250: 'GENERAL', 251: 'SCHEDULED CASTE', 252: 'SCHEDULED CASTE', 253: 'SCHEDULED CASTE', 254: 'SCHEDULED CASTE', 255: 'SCHEDULED CASTE', 256: 'SCHEDULED CASTE', 257: 'SCHEDULED CASTE', 258: 'SCHEDULED CASTE', 259: 'SCHEDULED CASTE', 260: 'SCHEDULED CASTE', 261: 'SCHEDULED CASTE', 262: 'SCHEDULED CASTE', 263: 'SCHEDULED CASTE', 264: 'SCHEDULED CASTE', 265: 'SCHEDULED CASTE', 266: 'SCHEDULED CASTE', 267: 'SCHEDULED CASTE', 268: 'SCHEDULED CASTE', 269: 'SCHEDULED CASTE', 270: 'SCHEDULED CASTE', 271: 'SCHEDULED CASTE', 272: 'SCHEDULED CASTE', 273: 'SCHEDULED CASTE', 274: 'SCHEDULED CASTE', 275: 'SCHEDULED CASTE', 276: 'SCHEDULED CASTE', 277: 'SCHEDULED CASTE', 278: 'SCHEDULED CASTE', 279: 'SCHEDULED CASTE', 280: 'SCHEDULED CASTE', 281: 'SCHEDULED CASTE', 282: 'SCHEDULED CASTE', 283: 'SCHEDULED CASTE', 284: 'SCHEDULED CASTE', 285: 'SCHEDULED CASTE', 286: 'SCHEDULED CASTE', 287: 'SCHEDULED CASTE', 288: 'SCHEDULED CASTE', 289: 'SCHEDULED CASTE', 290: 'SCHEDULED CASTE', 291: 'SCHEDULED CASTE', 292: 'SCHEDULED CASTE', 293: 'SCHEDULED CASTE', 294: 'SCHEDULED CASTE', 295: 'SCHEDULED CASTE', 296: 'SCHEDULED CASTE', 297: 'SCHEDULED CASTE', 298: 'SCHEDULED CASTE', 299: 'SCHEDULED CASTE', 300: 'SCHEDULED CASTE', 301: 'SCHEDULED CASTE', 302: 'SCHEDULED CASTE', 303: 'SCHEDULED CASTE', 304: 'SCHEDULED CASTE', 305: 'SCHEDULED CASTE', 306: 'SCHEDULED CASTE', 307: 'SCHEDULED CASTE', 308: 'SCHEDULED CASTE', 309: 'SCHEDULED CASTE', 310: 'SCHEDULED CASTE', 311: 'SCHEDULED CASTE', 312: 'SCHEDULED CASTE', 313: 'SCHEDULED CASTE', 314: 'SCHEDULED CASTE', 315: 'SCHEDULED CASTE', 316: 'SCHEDULED CASTE', 317: 'SCHEDULED CASTE', 318: 'SCHEDULED CASTE', 319: 'SCHEDULED CASTE', 320: 'SCHEDULED CASTE', 321: 'OBC', 322: 'OBC', 323: 'OBC', 324: 'OBC', 325: 'OBC', 326: 'OBC', 327: 'OBC', 328: 'OBC', 329: 'GENERAL', 330: 'GENERAL', 331: 'GENERAL', 332: 'GENERAL', 333: 'OBC', 334: 'GENERAL', 335: 'GENERAL', 336: 'GENERAL', 337: 'GENERAL', 338: 'OBC', 339: 'GENERAL', 340: 'GENERAL', 341: 'GENERAL', 342: 'GENERAL', 343: 'GENERAL', 344: 'GENERAL', 345: 'GENERAL', 346: 'GENERAL', 347: 'GENERAL', 348: 'GENERAL', 349: 'GENERAL', 350: 'GENERAL', 351: 'GENERAL', 352: 'GENERAL', 353: 'GENERAL', 354: 'GENERAL', 355: 'GENERAL', 356: 'GENERAL', 357: 'GENERAL', 358: 'GENERAL', 359: 'GENERAL', 360: 'GENERAL', 361: 'GENERAL', 362: 'GENERAL', 363: 'OBC', 364: 'GENERAL', 365: 'GENERAL', 366: 'GENERAL', 367: 'GENERAL', 368: 'GENERAL', 369: 'GENERAL', 370: 'GENERAL', 371: 'GENERAL', 372: 'GENERAL', 373: 'GENERAL', 374: 'GENERAL', 375: 'GENERAL', 376: 'GENERAL', 377: 'GENERAL', 378: 'GENERAL', 379: 'GENERAL', 380: 'GENERAL', 381: 'GENERAL', 382: 'GENERAL', 383: 'GENERAL', 384: 'GENERAL', 385: 'GENERAL', 386: 'GENERAL', 387: 'GENERAL', 388: 'GENERAL', 389: 'OBC', 390: 'OBC', 391: 'GENERAL', 392: 'GENERAL', 393: 'GENERAL', 394: 'GENERAL', 395: 'GENERAL', 396: 'GENERAL', 397: 'GENERAL', 398: 'OBC', 399: 'OBC', 400: 'GENERAL', 401: 'GENERAL', 402: 'GENERAL', 403: 'OBC', 404: 'GENERAL', 405: 'GENERAL'}\n",
      "Religion2: {203: 'HINDUISM', 204: 'HINDUISM', 205: 'HINDUISM', 206: 'HINDUISM', 207: 'HINDUISM', 208: 'HINDUISM', 209: 'HINDUISM', 210: 'HINDUISM', 211: 'HINDUISM', 212: 'HINDUISM', 213: 'HINDUISM', 214: 'HINDUISM', 215: 'HINDUISM', 216: 'HINDUISM', 217: 'HINDUISM', 218: 'HINDUISM', 219: 'HINDUISM', 220: 'HINDUISM', 221: 'HINDUISM', 222: 'HINDUISM', 223: 'HINDUISM', 224: 'HINDUISM', 225: 'HINDUISM', 226: 'HINDUISM', 227: 'HINDUISM', 228: 'HINDUISM', 229: 'HINDUISM', 230: 'HINDUISM', 231: 'HINDUISM', 232: 'HINDUISM', 233: 'HINDUISM', 234: 'HINDUISM', 235: 'HINDUISM', 236: 'HINDUISM', 237: 'HINDUISM', 238: 'HINDUISM', 239: 'HINDUISM', 240: 'HINDUISM', 241: 'HINDUISM', 242: 'HINDUISM', 243: 'HINDUISM', 244: 'HINDUISM', 245: 'HINDUISM', 246: 'HINDUISM', 247: 'HINDUISM', 248: 'HINDUISM', 249: 'HINDUISM', 250: 'HINDUISM', 251: 'HINDUISM', 252: 'HINDUISM', 253: 'HINDUISM', 254: 'HINDUISM', 255: 'HINDUISM', 256: 'HINDUISM', 257: 'HINDUISM', 258: 'HINDUISM', 259: 'HINDUISM', 260: 'HINDUISM', 261: 'HINDUISM', 262: 'HINDUISM', 263: 'HINDUISM', 264: 'HINDUISM', 265: 'HINDUISM', 266: 'HINDUISM', 267: 'HINDUISM', 268: 'HINDUISM', 269: 'HINDUISM', 270: 'HINDUISM', 271: 'HINDUISM', 272: 'HINDUISM', 273: 'HINDUISM', 274: 'HINDUISM', 275: 'HINDUISM', 276: 'HINDUISM', 277: 'HINDUISM', 278: 'HINDUISM', 279: 'HINDUISM', 280: 'HINDUISM', 281: 'HINDUISM', 282: 'HINDUISM', 283: 'HINDUISM', 284: 'HINDUISM', 285: 'HINDUISM', 286: 'HINDUISM', 287: 'HINDUISM', 288: 'HINDUISM', 289: 'HINDUISM', 290: 'HINDUISM', 291: 'HINDUISM', 292: 'HINDUISM', 293: 'HINDUISM', 294: 'HINDUISM', 295: 'HINDUISM', 296: 'HINDUISM', 297: 'HINDUISM', 298: 'HINDUISM', 299: 'HINDUISM', 300: 'HINDUISM', 301: 'HINDUISM', 302: 'HINDUISM', 303: 'HINDUISM', 304: 'HINDUISM', 305: 'HINDUISM', 306: 'HINDUISM', 307: 'HINDUISM', 308: 'HINDUISM', 309: 'HINDUISM', 310: 'HINDUISM', 311: 'HINDUISM', 312: 'HINDUISM', 313: 'HINDUISM', 314: 'HINDUISM', 315: 'HINDUISM', 316: 'HINDUISM', 317: 'HINDUISM', 318: 'HINDUISM', 319: 'HINDUISM', 320: 'HINDUISM', 321: 'HINDUISM', 322: 'HINDUISM', 323: 'HINDUISM', 324: 'HINDUISM', 325: 'HINDUISM', 326: 'HINDUISM', 327: 'HINDUISM', 328: 'HINDUISM', 329: 'HINDUISM', 330: 'HINDUISM', 331: 'HINDUISM', 332: 'HINDUISM', 333: 'HINDUISM', 334: 'HINDUISM', 335: 'HINDUISM', 336: 'HINDUISM', 337: 'HINDUISM', 338: 'HINDUISM', 339: 'HINDUISM', 340: 'HINDUISM', 341: 'HINDUISM', 342: 'HINDUISM', 343: 'HINDUISM', 344: 'HINDUISM', 345: 'HINDUISM', 346: 'HINDUISM', 347: 'HINDUISM', 348: 'HINDUISM', 349: 'HINDUISM', 350: 'HINDUISM', 351: 'HINDUISM', 352: 'HINDUISM', 353: 'HINDUISM', 354: 'HINDUISM', 355: 'HINDUISM', 356: 'HINDUISM', 357: 'HINDUISM', 358: 'HINDUISM', 359: 'HINDUISM', 360: 'HINDUISM', 361: 'HINDUISM', 362: 'HINDUISM', 363: 'HINDUISM', 364: 'HINDUISM', 365: 'HINDUISM', 366: 'HINDUISM', 367: 'HINDUISM', 368: 'HINDUISM', 369: 'HINDUISM', 370: 'HINDUISM', 371: 'HINDUISM', 372: 'HINDUISM', 373: 'HINDUISM', 374: 'HINDUISM', 375: 'HINDUISM', 376: 'HINDUISM', 377: 'HINDUISM', 378: 'HINDUISM', 379: 'HINDUISM', 380: 'HINDUISM', 381: 'HINDUISM', 382: 'HINDUISM', 383: 'HINDUISM', 384: 'HINDUISM', 385: 'HINDUISM', 386: 'HINDUISM', 387: 'HINDUISM', 388: 'HINDUISM', 389: 'HINDUISM', 390: 'HINDUISM', 391: 'HINDUISM', 392: 'HINDUISM', 393: 'HINDUISM', 394: 'HINDUISM', 395: 'HINDUISM', 396: 'HINDUISM', 397: 'HINDUISM', 398: 'HINDUISM', 399: 'HINDUISM', 400: 'HINDUISM', 401: 'HINDUISM', 402: 'HINDUISM', 403: 'HINDUISM', 404: 'HINDUISM', 405: 'HINDUISM'}\n"
     ]
    }
   ],
   "source": [
    "# Create dictionaries for Village 1\n",
    "sex1 = df1['resp_gend'].to_dict()\n",
    "caste1 = df1['caste'].to_dict()\n",
    "religion1 = df1['religion'].to_dict()\n",
    "\n",
    "# Create dictionaries for Village 2\n",
    "sex2 = df2['resp_gend'].to_dict()\n",
    "caste2 = df2['caste'].to_dict()\n",
    "religion2 = df2['religion'].to_dict()\n",
    "\n",
    "# Display the first few entries of df1 to verify\n",
    "print(df1.head())\n",
    "\n",
    "# Displaying the created dictionaries (optional, can be commented out)\n",
    "print(\"Sex1:\", sex1)\n",
    "print(\"Caste1:\", caste1)\n",
    "print(\"Religion1:\", religion1)\n",
    "print(\"Sex2:\", sex2)\n",
    "print(\"Caste2:\", caste2)\n",
    "print(\"Religion2:\", religion2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "\n",
    "In this exercise, we will print the chance homophily of several characteristics of Villages 1 and 2. \n",
    "\n",
    "#### Instructions \n",
    "-  Use `chance_homophily` to compute the chance homophily for sex, caste, and religion In Villages 1 and 2. Is the chance homophily for any attribute very high for either village?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chance_homophily(chars):\n",
    "    marginal_probs = marginal_prob(chars)\n",
    "    homophily = sum(prob ** 2 for prob in marginal_probs.values())\n",
    "    return homophily"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "\n",
    "In this exercise, we will create a function that computes the observed homophily given a village and characteristic.\n",
    "\n",
    "#### Instructions \n",
    "- Complete the function `homophily()`, which takes a network `G`, a dictionary of node characteristics `chars`, and node IDs `IDs`. For each node pair, determine whether a tie exists between them, as well as whether they share a characteristic. The total count of these is `num_ties` and `num_same_ties`, respectively, and their ratio is the homophily of chars in `G`. Complete the function by choosing where to increment `num_same_ties` and `num_ties`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homophily: 0.2\n"
     ]
    }
   ],
   "source": [
    "def homophily(G, chars, IDs):\n",
    "    \"\"\"\n",
    "    Given a network G, a dict of characteristics chars for node IDs,\n",
    "    and dict of node IDs for each node in the network,\n",
    "    find the homophily of the network.\n",
    "    \"\"\"\n",
    "    num_same_ties = 0\n",
    "    num_ties = 0\n",
    "    for n1, n2 in G.edges():\n",
    "        if n1 in IDs and n2 in IDs:\n",
    "            if IDs[n1] in chars and IDs[n2] in chars:\n",
    "                num_ties += 1  # Increment the count of ties\n",
    "                if chars[IDs[n1]] == chars[IDs[n2]]:\n",
    "                    num_same_ties += 1  # Increment the count of same characteristic ties\n",
    "                    \n",
    "    if num_ties == 0:  # To handle the case where there are no ties\n",
    "        return 0\n",
    "    \n",
    "    return num_same_ties / num_ties\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Create a sample network\n",
    "G = nx.Graph()\n",
    "G.add_edges_from([(1, 2), (2, 3), (3, 4), (4, 1), (1, 3)])\n",
    "\n",
    "# Sample IDs and characteristics\n",
    "IDs = {1: \"a\", 2: \"b\", 3: \"c\", 4: \"d\"}\n",
    "chars = {\"a\": \"red\", \"b\": \"blue\", \"c\": \"red\", \"d\": \"blue\"}\n",
    "\n",
    "# Calculate homophily\n",
    "homophily_score = homophily(G, chars, IDs)\n",
    "print(\"Homophily:\", homophily_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "\n",
    "In this exercise, we will obtain the personal IDs for Villages 1 and 2. These will be used in the next exercise to calculate homophily for these villages.\n",
    "\n",
    "#### Instructions \n",
    "- In this dataset, each individual has a personal ID, or PID, stored in `key_vilno_1.csv` and `key_vilno_2.csv` for villages 1 and 2, respectively. `data_filepath1` and `data_filepath2` contain the URLs to the datasets used in this exercise. Use `pd.read_csv` to read in and store `key_vilno_1.csv` and `key_vilno_2.csv` as `pid1` and `pid2` respectively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Village 1 PID data:\n",
      "   Unnamed: 0       0\n",
      "0           0  100101\n",
      "1           1  100102\n",
      "2           2  100103\n",
      "3           3  100104\n",
      "4           4  100201\n",
      "\n",
      "Village 2 PID data:\n",
      "   Unnamed: 0       0\n",
      "0           0  200101\n",
      "1           1  200102\n",
      "2           2  200103\n",
      "3           3  200104\n",
      "4           4  200105\n"
     ]
    }
   ],
   "source": [
    "data_filepath1 = \"https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@key_vilno_1.csv\"\n",
    "data_filepath2 = \"https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@key_vilno_2.csv\"\n",
    "\n",
    "# Enter code here!\n",
    "pid1 = pd.read_csv(data_filepath1)\n",
    "pid2 = pd.read_csv(data_filepath2)\n",
    "\n",
    "# Display the first few entries of each to verify\n",
    "print(\"Village 1 PID data:\")\n",
    "print(pid1.head())\n",
    "\n",
    "print(\"\\nVillage 2 PID data:\")\n",
    "print(pid2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "\n",
    "In this exercise, we will compute the homophily of several network characteristics for Villages 1 and 2 and compare them to homophily due to chance alone. The networks for these villages have been stored as networkx graph objects `G1` and `G2`.\n",
    "\n",
    "#### Instructions \n",
    "\n",
    "- Use your `homophily()` function to compute the observed homophily for sex, caste, and religion in Villages 1 and 2. Print all six values.\n",
    "- Use the `chance_homophily()` to compare these values to chance homophily. Are these values higher or lower than that expected by chance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Village 1 - Observed Homophily\n",
      "Sex Homophily: 0\n",
      "Caste Homophily: 0\n",
      "Religion Homophily: 0\n",
      "\n",
      "Village 1 - Chance Homophily\n",
      "Sex Homophily: 0.5027299861680701\n",
      "Caste Homophily: 0.6741488509791551\n",
      "Religion Homophily: 0.9804896988521925\n",
      "\n",
      "Village 2 - Observed Homophily\n",
      "Sex Homophily: 0\n",
      "Caste Homophily: 0\n",
      "Religion Homophily: 0\n",
      "\n",
      "Village 2 - Chance Homophily\n",
      "Sex Homophily: 0.5005945303210464\n",
      "Caste Homophily: 0.425368244800893\n",
      "Religion Homophily: 1.0\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "A1 = np.array(pd.read_csv(\"https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@adj_allVillageRelationships_vilno1.csv\", index_col=0))\n",
    "A2 = np.array(pd.read_csv(\"https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@adj_allVillageRelationships_vilno2.csv\", index_col=0))\n",
    "G1 = nx.to_networkx_graph(A1)\n",
    "G2 = nx.to_networkx_graph(A2)\n",
    "\n",
    "pid1 = pd.read_csv(data_filepath1, dtype=int)['0'].to_dict()\n",
    "pid2 = pd.read_csv(data_filepath2, dtype=int)['0'].to_dict()\n",
    "\n",
    "char_filepath = \"https://courses.edx.org/asset-v1:HarvardX+PH526x+2T2019+type@asset+block@individual_characteristics.csv\"\n",
    "df = pd.read_csv(char_filepath, low_memory=False, index_col=0)\n",
    "# Filter the dataset for Villages 1 and 2\n",
    "df1 = df[df['village'] == 1]\n",
    "df2 = df[df['village'] == 2]\n",
    "\n",
    "# Enter your code here!\n",
    "observed_sex_homophily_v1 = homophily(G1, sex1, pid1)\n",
    "observed_caste_homophily_v1 = homophily(G1, caste1, pid1)\n",
    "observed_religion_homophily_v1 = homophily(G1, religion1, pid1)\n",
    "\n",
    "# Compute observed homophily for sex, caste, and religion in Village 2\n",
    "observed_sex_homophily_v2 = homophily(G2, sex2, pid2)\n",
    "observed_caste_homophily_v2 = homophily(G2, caste2, pid2)\n",
    "observed_religion_homophily_v2 = homophily(G2, religion2, pid2)\n",
    "\n",
    "# Compute chance homophily for sex, caste, and religion in Village 1\n",
    "chance_sex_homophily_v1 = chance_homophily(sex1)\n",
    "chance_caste_homophily_v1 = chance_homophily(caste1)\n",
    "chance_religion_homophily_v1 = chance_homophily(religion1)\n",
    "\n",
    "# Compute chance homophily for sex, caste, and religion in Village 2\n",
    "chance_sex_homophily_v2 = chance_homophily(sex2)\n",
    "chance_caste_homophily_v2 = chance_homophily(caste2)\n",
    "chance_religion_homophily_v2 = chance_homophily(religion2)\n",
    "\n",
    "# Print the results\n",
    "print(\"Village 1 - Observed Homophily\")\n",
    "print(\"Sex Homophily:\", observed_sex_homophily_v1)\n",
    "print(\"Caste Homophily:\", observed_caste_homophily_v1)\n",
    "print(\"Religion Homophily:\", observed_religion_homophily_v1)\n",
    "\n",
    "print(\"\\nVillage 1 - Chance Homophily\")\n",
    "print(\"Sex Homophily:\", chance_sex_homophily_v1)\n",
    "print(\"Caste Homophily:\", chance_caste_homophily_v1)\n",
    "print(\"Religion Homophily:\", chance_religion_homophily_v1)\n",
    "\n",
    "print(\"\\nVillage 2 - Observed Homophily\")\n",
    "print(\"Sex Homophily:\", observed_sex_homophily_v2)\n",
    "print(\"Caste Homophily:\", observed_caste_homophily_v2)\n",
    "print(\"Religion Homophily:\", observed_religion_homophily_v2)\n",
    "\n",
    "print(\"\\nVillage 2 - Chance Homophily\")\n",
    "print(\"Sex Homophily:\", chance_sex_homophily_v2)\n",
    "print(\"Caste Homophily:\", chance_caste_homophily_v2)\n",
    "print(\"Religion Homophily:\", chance_religion_homophily_v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
